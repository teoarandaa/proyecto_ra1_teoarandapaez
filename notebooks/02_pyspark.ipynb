{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6692ccfb",
   "metadata": {},
   "source": [
    "# ETL: Limpieza de Datos de Viviendas en Barcelona (PySpark)\n",
    "\n",
    "## Importamos las librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "308567e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession creada correctamente\n",
      "Versi√≥n de Spark: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "\n",
    "# Crear SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Housing_Barcelona\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ SparkSession creada correctamente\")\n",
    "print(f\"Versi√≥n de Spark: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61fccc",
   "metadata": {},
   "source": [
    "## EXTRACT: Cargar los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d18d7f",
   "metadata": {},
   "source": [
    "## EXPLORACI√ìN INICIAL DEL DATASET\n",
    "\n",
    "### An√°lisis descriptivo y detecci√≥n de problemas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "961281fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ El dataframe se ha creado correctamente\n",
      "Filas: 10000\n",
      "Columnas: 20\n",
      "\n",
      "Columnas: ['listing_id', 'operation', 'district', 'neighborhood', 'address', 'surface_m2', 'rooms', 'bathrooms', 'price_eur', 'price_per_m2', 'floor', 'elevator', 'balcony', 'furnished', 'condition', 'energy_certificate', 'has_parking', 'latitude', 'longitude', 'agency']\n",
      "\n",
      "Primeras filas:\n",
      "+----------+---------+----------+---------------+-----------------+----------+-----+---------+---------+------------+------+--------+-------+---------+----------+------------------+-----------+--------+---------+---------------+\n",
      "|listing_id|operation|district  |neighborhood   |address          |surface_m2|rooms|bathrooms|price_eur|price_per_m2|floor |elevator|balcony|furnished|condition |energy_certificate|has_parking|latitude|longitude|agency         |\n",
      "+----------+---------+----------+---------------+-----------------+----------+-----+---------+---------+------------+------+--------+-------+---------+----------+------------------+-----------+--------+---------+---------------+\n",
      "|ID_0      |alquiler |Unknown   |Sagrada Fam√≠lia|C/ Arag√≥ 395     |89 m¬≤     |?    |2        |?        |4240 ‚Ç¨/m2   |1¬∫    |Y       |No     |partially|average   |?                 |No         |N/A     |?        |Particular     |\n",
      "|NULL      |VENDER   |Eixampl   |Les Corts      |Passeig de Gr√†cia|171       |N/A  |1        |?        |7920.91     |√°tico |?       |N      |N/A      |?         |D                 |No         |N/A     |?        |Housfy         |\n",
      "|ID_2      |lease    |Sant Mart√≠|El Clot        |C/ Mallorca 316  |?         |2+   |?        |317642 ‚Ç¨ |?           |2¬∫    |Y       |?      |?        |average   |D                 |?          |41.3997 |?        |Engel & V√∂lkers|\n",
      "|NULL      |alquiler |SANTS     |Sagrada Fam√≠lia|Calle Falsa 123  |N/A       |three|two      |N/A      |5484 ‚Ç¨/m2   |s√≥tano|N       |S√≠     |?        |a reformar|A                 |Y          |N/A     |2.0      |Engel & V√∂lkers|\n",
      "|5         |buy      |SANTS     |Les Corts      |C/ Gran Via 245  |?         |2+   |?        |N/A      |?           |4¬∫    |S√≠      |N      |S√≠       |average   |F                 |Y          |?       |2.0      |Particular     |\n",
      "+----------+---------+----------+---------------+-----------------+----------+-----+---------+---------+------------+------+--------+-------+---------+----------+------------------+-----------+--------+---------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Cargar el CSV\n",
    "df_raw = spark.read.csv(\n",
    "    \"../data/housing-barcelona.csv\",\n",
    "    header=True,\n",
    "    inferSchema=False  # Leer todo como string primero\n",
    ")\n",
    "\n",
    "print(\"‚úÖ El dataframe se ha creado correctamente\")\n",
    "print(f\"Filas: {df_raw.count()}\")\n",
    "print(f\"Columnas: {len(df_raw.columns)}\")\n",
    "print(f\"\\nColumnas: {df_raw.columns}\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df_raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56bd0d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INFORMACI√ìN GENERAL DEL DATASET ===\n",
      "\n",
      "Dimensiones: 10000 filas √ó 20 columnas\n",
      "\n",
      "Esquema original:\n",
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- surface_m2: string (nullable = true)\n",
      " |-- rooms: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- price_eur: string (nullable = true)\n",
      " |-- price_per_m2: string (nullable = true)\n",
      " |-- floor: string (nullable = true)\n",
      " |-- elevator: string (nullable = true)\n",
      " |-- balcony: string (nullable = true)\n",
      " |-- furnished: string (nullable = true)\n",
      " |-- condition: string (nullable = true)\n",
      " |-- energy_certificate: string (nullable = true)\n",
      " |-- has_parking: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- agency: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Informaci√≥n general del dataset\n",
    "print(\"=== INFORMACI√ìN GENERAL DEL DATASET ===\\n\")\n",
    "total_rows = df_raw.count()\n",
    "total_cols = len(df_raw.columns)\n",
    "print(f\"Dimensiones: {total_rows} filas √ó {total_cols} columnas\")\n",
    "print(f\"\\nEsquema original:\")\n",
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86b1fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AN√ÅLISIS DE VALORES FALTANTES ===\n",
      "\n",
      "  latitude: 5055 valores faltantes (50.55%)\n",
      "  price_per_m2: 5041 valores faltantes (50.41%)\n",
      "  longitude: 4919 valores faltantes (49.19%)\n",
      "  rooms: 4004 valores faltantes (40.04%)\n",
      "  bathrooms: 3992 valores faltantes (39.92%)\n",
      "  surface_m2: 3983 valores faltantes (39.83%)\n",
      "  furnished: 3907 valores faltantes (39.07%)\n",
      "  price_eur: 3347 valores faltantes (33.47%)\n",
      "  listing_id: 3321 valores faltantes (33.21%)\n",
      "  balcony: 2579 valores faltantes (25.79%)\n",
      "  condition: 2533 valores faltantes (25.33%)\n",
      "  elevator: 2451 valores faltantes (24.51%)\n",
      "  has_parking: 2447 valores faltantes (24.47%)\n",
      "  energy_certificate: 2234 valores faltantes (22.34%)\n",
      "  address: 1966 valores faltantes (19.66%)\n",
      "  neighborhood: 1425 valores faltantes (14.25%)\n",
      "  agency: 1422 valores faltantes (14.22%)\n",
      "  operation: 1399 valores faltantes (13.99%)\n",
      "  floor: 1277 valores faltantes (12.77%)\n",
      "\n",
      "Total de valores faltantes detectados: 57302\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de valores faltantes\n",
    "print(\"=== AN√ÅLISIS DE VALORES FALTANTES ===\\n\")\n",
    "from pyspark.sql.functions import col, when, isnan, isnull, count\n",
    "import builtins\n",
    "\n",
    "# Calcular total_rows si no est√° definido (por si se ejecuta esta celda antes que la anterior)\n",
    "if 'total_rows' not in globals():\n",
    "    total_rows = df_raw.count()\n",
    "\n",
    "\n",
    "null_counts = {}\n",
    "for col_name in df_raw.columns:\n",
    "    null_count = df_raw.filter(col(col_name).isNull() | (col(col_name) == '') | (col(col_name) == '?') | \n",
    "                                (col(col_name) == 'N/A') | (col(col_name) == 'NULL') | \n",
    "                                (col(col_name) == 'null') | (col(col_name) == 'unknown')).count()\n",
    "    if null_count > 0:\n",
    "        null_counts[col_name] = null_count\n",
    "\n",
    "# Ordenar por cantidad de valores faltantes\n",
    "sorted_null_counts = sorted(null_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for col_name, count_val in sorted_null_counts:\n",
    "    percentage = (count_val / total_rows) * 100\n",
    "    print(f\"  {col_name}: {count_val} valores faltantes ({percentage:.2f}%)\")\n",
    "\n",
    "total_missing = builtins.sum(null_counts.values())\n",
    "print(f\"\\nTotal de valores faltantes detectados: {total_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cce6bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AN√ÅLISIS DE VALORES √öNICOS Y DUPLICADOS ===\n",
      "\n",
      "Filas duplicadas: 0\n",
      "\n",
      "Valores √∫nicos por columna:\n",
      "  listing_id: 6680 valores √∫nicos\n",
      "  operation: 7 valores √∫nicos\n",
      "  district: 14 valores √∫nicos\n",
      "  neighborhood: 14 valores √∫nicos\n",
      "  address: 1148 valores √∫nicos\n",
      "  surface_m2: 405 valores √∫nicos\n",
      "  rooms: 10 valores √∫nicos\n",
      "  bathrooms: 7 valores √∫nicos\n",
      "  price_eur: 4644 valores √∫nicos\n",
      "  price_per_m2: 4427 valores √∫nicos\n",
      "  floor: 8 valores √∫nicos\n",
      "  elevator: 8 valores √∫nicos\n",
      "  balcony: 8 valores √∫nicos\n",
      "  furnished: 5 valores √∫nicos\n",
      "  condition: 8 valores √∫nicos\n",
      "  energy_certificate: 9 valores √∫nicos\n",
      "  has_parking: 8 valores √∫nicos\n",
      "  latitude: 2108 valores √∫nicos\n",
      "  longitude: 2362 valores √∫nicos\n",
      "  agency: 7 valores √∫nicos\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de valores √∫nicos y duplicados\n",
    "print(\"=== AN√ÅLISIS DE VALORES √öNICOS Y DUPLICADOS ===\\n\")\n",
    "duplicate_count = df_raw.count() - df_raw.dropDuplicates().count()\n",
    "print(f\"Filas duplicadas: {duplicate_count}\")\n",
    "\n",
    "print(f\"\\nValores √∫nicos por columna:\")\n",
    "for col_name in df_raw.columns:\n",
    "    unique_count = df_raw.select(col_name).distinct().count()\n",
    "    print(f\"  {col_name}: {unique_count} valores √∫nicos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69e1b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETECCI√ìN DE VALORES PROBLEM√ÅTICOS ===\n",
      "\n",
      "operation: 1399 valores problem√°ticos detectados\n",
      "district: 686 valores problem√°ticos detectados\n",
      "neighborhood: 1425 valores problem√°ticos detectados\n",
      "address: 1966 valores problem√°ticos detectados\n",
      "surface_m2: 3983 valores problem√°ticos detectados\n",
      "rooms: 4004 valores problem√°ticos detectados\n",
      "bathrooms: 3992 valores problem√°ticos detectados\n",
      "price_eur: 3347 valores problem√°ticos detectados\n",
      "price_per_m2: 5041 valores problem√°ticos detectados\n",
      "floor: 1277 valores problem√°ticos detectados\n",
      "elevator: 2451 valores problem√°ticos detectados\n",
      "balcony: 2579 valores problem√°ticos detectados\n",
      "furnished: 3907 valores problem√°ticos detectados\n",
      "condition: 2533 valores problem√°ticos detectados\n",
      "energy_certificate: 2234 valores problem√°ticos detectados\n",
      "has_parking: 2447 valores problem√°ticos detectados\n",
      "latitude: 5055 valores problem√°ticos detectados\n",
      "longitude: 4919 valores problem√°ticos detectados\n",
      "agency: 1422 valores problem√°ticos detectados\n",
      "\n",
      "Verificando espacios en blanco:\n",
      "  neighborhood: 739 valores con espacios al inicio/final\n",
      "  address: 1938 valores con espacios al inicio/final\n"
     ]
    }
   ],
   "source": [
    "# Detecci√≥n de valores problem√°ticos\n",
    "print(\"=== DETECCI√ìN DE VALORES PROBLEM√ÅTICOS ===\\n\")\n",
    "valores_problematicos = ['?', 'N/A', 'n/a', 'NULL', 'null', 'unknown', 'Unknown', '']\n",
    "\n",
    "for col_name in df_raw.columns:\n",
    "    problematic_count = df_raw.filter(col(col_name).isin(valores_problematicos)).count()\n",
    "    if problematic_count > 0:\n",
    "        print(f\"{col_name}: {problematic_count} valores problem√°ticos detectados\")\n",
    "        \n",
    "# Verificar espacios en blanco al inicio/final (usando trim)\n",
    "print(\"\\nVerificando espacios en blanco:\")\n",
    "for col_name in df_raw.columns:\n",
    "    # Contar filas donde el valor trimado es diferente al original\n",
    "    has_spaces = df_raw.filter(trim(col(col_name)) != col(col_name)).count()\n",
    "    if has_spaces > 0:\n",
    "        print(f\"  {col_name}: {has_spaces} valores con espacios al inicio/final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219bdbd",
   "metadata": {},
   "source": [
    "## TRANSFORM: Limpieza de Datos\n",
    "\n",
    "### Paso 1: Crear copia para trabajar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa981883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataframe listo para trabajar. Filas: 10000\n"
     ]
    }
   ],
   "source": [
    "# Crear copia del dataframe (en PySpark, los DataFrames son inmutables, as√≠ que trabajamos directamente)\n",
    "df_clean = df_raw\n",
    "print(f\"‚úÖ Dataframe listo para trabajar. Filas: {df_clean.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a025561",
   "metadata": {},
   "source": [
    "### Paso 2: Eliminar espacios (strip) en columnas de texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1786c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Espacios eliminados de todas las columnas\n"
     ]
    }
   ],
   "source": [
    "# Aplicar trim() a todas las columnas (elimina espacios al inicio y final)\n",
    "for col_name in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "print(\"‚úÖ Espacios eliminados de todas las columnas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfdee46",
   "metadata": {},
   "source": [
    "### Paso 3: Reemplazar valores vac√≠os por NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35382f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Valores vac√≠os convertidos a NULL\n",
      "\n",
      "Valores NULL por columna:\n",
      "  latitude: 5055\n",
      "  price_per_m2: 5041\n",
      "  longitude: 4919\n",
      "  rooms: 4004\n",
      "  bathrooms: 3992\n",
      "  surface_m2: 3983\n",
      "  furnished: 3907\n",
      "  address: 3904\n",
      "  price_eur: 3347\n",
      "  listing_id: 3321\n",
      "  balcony: 2579\n",
      "  condition: 2533\n",
      "  elevator: 2451\n",
      "  has_parking: 2447\n",
      "  energy_certificate: 2234\n",
      "  neighborhood: 1425\n",
      "  agency: 1422\n",
      "  operation: 1399\n",
      "  floor: 1277\n"
     ]
    }
   ],
   "source": [
    "# Reemplazar valores que representan \"vac√≠o\" por NULL (None en PySpark)\n",
    "valores_vacios = ['', ' ', 'nan', 'None', 'N/A', 'n/a', 'NULL', 'null', '?', 'unknown']\n",
    "\n",
    "for col_name in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name).isin(valores_vacios) | col(col_name).isNull(), None)\n",
    "        .otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Valores vac√≠os convertidos a NULL\")\n",
    "print(f\"\\nValores NULL por columna:\")\n",
    "null_counts = {}\n",
    "for col_name in df_clean.columns:\n",
    "    null_count = df_clean.filter(col(col_name).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        null_counts[col_name] = null_count\n",
    "\n",
    "for col_name, count in sorted(null_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {col_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95910e0",
   "metadata": {},
   "source": [
    "### Paso 4: Convertir tipos de datos adecuados\n",
    "\n",
    "Primero definimos UDFs (User Defined Functions) para las funciones personalizadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f4ee387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ UDFs definidas y registradas\n"
     ]
    }
   ],
   "source": [
    "# Definir UDFs para extraer n√∫meros y convertir texto\n",
    "\n",
    "# UDF para extraer n√∫meros de strings\n",
    "def extract_number_udf(value):\n",
    "    \"\"\"Extrae el primer n√∫mero de un string\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    value_str = str(value)\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', value_str)\n",
    "    if numbers:\n",
    "        return float(numbers[0])\n",
    "    return None\n",
    "\n",
    "# UDF para convertir texto a n√∫mero\n",
    "def text_to_number_udf(value):\n",
    "    \"\"\"Convierte texto a n√∫mero\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    value_str = str(value).lower().strip()\n",
    "    \n",
    "    text_map = {\n",
    "        'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
    "        'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10\n",
    "    }\n",
    "    \n",
    "    if value_str in text_map:\n",
    "        return text_map[value_str]\n",
    "    \n",
    "    if '+' in value_str:\n",
    "        nums = re.findall(r'\\d+', value_str)\n",
    "        if nums:\n",
    "            return int(nums[0])\n",
    "    \n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', value_str)\n",
    "    if numbers:\n",
    "        return float(numbers[0])\n",
    "    return None\n",
    "\n",
    "# UDF para extraer precio\n",
    "def extract_price_udf(value):\n",
    "    \"\"\"Extrae precio num√©rico\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    value_str = str(value).replace('‚Ç¨', '').replace('.', '').replace(',', '.').strip()\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', value_str)\n",
    "    if numbers:\n",
    "        return float(numbers[0])\n",
    "    return None\n",
    "\n",
    "# UDF para extraer precio por m¬≤\n",
    "def extract_price_m2_udf(value):\n",
    "    \"\"\"Extrae precio por m¬≤\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    value_str = str(value).replace('‚Ç¨/m2', '').replace('‚Ç¨/m¬≤', '').replace('.', '').replace(',', '.').strip()\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', value_str)\n",
    "    if numbers:\n",
    "        return float(numbers[0])\n",
    "    return None\n",
    "\n",
    "# Registrar UDFs\n",
    "extract_number = udf(extract_number_udf, DoubleType())\n",
    "text_to_number = udf(text_to_number_udf, DoubleType())\n",
    "extract_price = udf(extract_price_udf, DoubleType())\n",
    "extract_price_m2 = udf(extract_price_m2_udf, DoubleType())\n",
    "\n",
    "print(\"‚úÖ UDFs definidas y registradas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a9af2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columnas num√©ricas limpiadas y convertidas\n",
      "\n",
      "Tipos de datos num√©ricos:\n",
      "root\n",
      " |-- surface_m2: double (nullable = true)\n",
      " |-- rooms: double (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- price_eur: double (nullable = true)\n",
      " |-- price_per_m2: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Limpiar columnas num√©ricas usando las UDFs\n",
    "\n",
    "# Limpiar surface_m2\n",
    "if 'surface_m2' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('surface_m2', extract_number('surface_m2'))\n",
    "\n",
    "# Limpiar rooms\n",
    "if 'rooms' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('rooms', text_to_number('rooms'))\n",
    "\n",
    "# Limpiar bathrooms\n",
    "if 'bathrooms' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('bathrooms', text_to_number('bathrooms'))\n",
    "\n",
    "# Limpiar price_eur\n",
    "if 'price_eur' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('price_eur', extract_price('price_eur'))\n",
    "\n",
    "# Limpiar price_per_m2\n",
    "if 'price_per_m2' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('price_per_m2', extract_price_m2('price_per_m2'))\n",
    "\n",
    "# Convertir coordenadas a num√©rico\n",
    "if 'latitude' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('latitude', col('latitude').cast('double'))\n",
    "if 'longitude' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('longitude', col('longitude').cast('double'))\n",
    "\n",
    "print(\"‚úÖ Columnas num√©ricas limpiadas y convertidas\")\n",
    "print(f\"\\nTipos de datos num√©ricos:\")\n",
    "df_clean.select('surface_m2', 'rooms', 'bathrooms', 'price_eur', 'price_per_m2', 'latitude', 'longitude').printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff669aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columnas convertidas a enteros\n"
     ]
    }
   ],
   "source": [
    "# Convertir columnas que deben ser enteros\n",
    "if 'rooms' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('rooms', col('rooms').cast('int'))\n",
    "if 'bathrooms' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('bathrooms', col('bathrooms').cast('int'))\n",
    "\n",
    "print(\"‚úÖ Columnas convertidas a enteros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03e465fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columnas de texto convertidas a string\n"
     ]
    }
   ],
   "source": [
    "# Asegurar que las columnas de texto sean string\n",
    "text_cols = ['listing_id', 'operation', 'district', 'neighborhood', 'address', \n",
    "             'floor', 'condition', 'energy_certificate', 'agency']\n",
    "\n",
    "for col_name in text_cols:\n",
    "    if col_name in df_clean.columns:\n",
    "        df_clean = df_clean.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull() | (col(col_name) == 'nan'), None)\n",
    "            .otherwise(col(col_name).cast('string'))\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Columnas de texto convertidas a string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a475f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columnas booleanas convertidas\n"
     ]
    }
   ],
   "source": [
    "# Convertir columnas booleanas\n",
    "boolean_cols = ['elevator', 'balcony', 'furnished', 'has_parking']\n",
    "\n",
    "for col_name in boolean_cols:\n",
    "    if col_name in df_clean.columns:\n",
    "        df_clean = df_clean.withColumn(\n",
    "            col_name,\n",
    "            when(lower(trim(col(col_name))).isin(['y', 'yes', 's√≠', 'si', 's', '1', 'true']), True)\n",
    "            .when(lower(trim(col(col_name))).isin(['n', 'no', '0', 'false']), False)\n",
    "            .otherwise(None)\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Columnas booleanas convertidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa0fa6",
   "metadata": {},
   "source": [
    "### Paso 5: Rellenar valores faltantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "310ad9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RELLENANDO VALORES FALTANTES ===\n",
      "\n",
      "‚úÖ listing_id: valores rellenados con 'listing_id empty'\n",
      "‚úÖ operation: valores rellenados con 'operation empty'\n",
      "‚úÖ district: valores rellenados con 'district empty'\n",
      "‚úÖ neighborhood: valores rellenados con 'neighborhood empty'\n",
      "‚úÖ address: valores rellenados con 'address empty'\n",
      "‚úÖ floor: valores rellenados con 'floor empty'\n",
      "‚úÖ condition: valores rellenados con 'condition empty'\n",
      "‚úÖ energy_certificate: valores rellenados con 'energy_certificate empty'\n",
      "‚úÖ agency: valores rellenados con 'agency empty'\n",
      "‚úÖ surface_m2: valores rellenados con media = 106.58\n",
      "‚úÖ rooms: valores rellenados con media = 3 (entero)\n",
      "‚úÖ bathrooms: valores rellenados con media = 1 (entero)\n",
      "‚úÖ price_eur: valores rellenados con media = 263348.96\n",
      "‚úÖ price_per_m2: valores rellenados con media = 281963.15\n",
      "‚úÖ latitude: valores rellenados con media = 41.19\n",
      "‚úÖ longitude: valores rellenados con media = 2.08\n",
      "‚úÖ elevator: valores rellenados con False\n",
      "‚úÖ balcony: valores rellenados con False\n",
      "‚úÖ furnished: valores rellenados con False\n",
      "‚úÖ has_parking: valores rellenados con False\n",
      "\n",
      "‚úÖ Todos los valores faltantes han sido rellenados\n",
      "Valores NULL restantes: 0\n"
     ]
    }
   ],
   "source": [
    "# Rellenar valores faltantes\n",
    "# Para columnas de texto: rellenar con \"{nombre_columna} empty\"\n",
    "# Para columnas num√©ricas: rellenar con la media\n",
    "# Para columnas booleanas: rellenar con False\n",
    "\n",
    "import builtins  # Para usar round() de Python en lugar del de PySpark\n",
    "\n",
    "print(\"=== RELLENANDO VALORES FALTANTES ===\\n\")\n",
    "\n",
    "# Obtener esquema para identificar tipos\n",
    "schema = df_clean.schema\n",
    "\n",
    "# Crear diccionario para fillna\n",
    "fill_dict = {}\n",
    "\n",
    "# Rellenar columnas de texto (StringType)\n",
    "for field in schema.fields:\n",
    "    if isinstance(field.dataType, StringType):\n",
    "        fill_dict[field.name] = f\"{field.name} empty\"\n",
    "        print(f\"‚úÖ {field.name}: valores rellenados con '{fill_dict[field.name]}'\")\n",
    "\n",
    "# Rellenar columnas num√©ricas con la media\n",
    "numeric_cols = ['surface_m2', 'rooms', 'bathrooms', 'price_eur', 'price_per_m2', 'latitude', 'longitude']\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    if col_name in df_clean.columns:\n",
    "        # Calcular media\n",
    "        mean_value = df_clean.agg(avg(col(col_name)).alias('mean')).collect()[0]['mean']\n",
    "        \n",
    "        if mean_value is not None:\n",
    "            # Si es int, redondear usando round() de Python (builtins)\n",
    "            if col_name in ['rooms', 'bathrooms']:\n",
    "                mean_value = int(builtins.round(mean_value))\n",
    "                fill_dict[col_name] = mean_value\n",
    "                print(f\"‚úÖ {col_name}: valores rellenados con media = {mean_value} (entero)\")\n",
    "            else:\n",
    "                fill_dict[col_name] = mean_value\n",
    "                print(f\"‚úÖ {col_name}: valores rellenados con media = {mean_value:.2f}\")\n",
    "\n",
    "# Rellenar columnas booleanas con False\n",
    "for field in schema.fields:\n",
    "    if isinstance(field.dataType, BooleanType):\n",
    "        fill_dict[field.name] = False\n",
    "        print(f\"‚úÖ {field.name}: valores rellenados con False\")\n",
    "\n",
    "# Aplicar fillna\n",
    "df_clean = df_clean.fillna(fill_dict)\n",
    "\n",
    "# Verificar valores NULL restantes\n",
    "null_count = 0\n",
    "for col_name in df_clean.columns:\n",
    "    col_null_count = df_clean.filter(col(col_name).isNull()).count()\n",
    "    if col_null_count > 0:\n",
    "        null_count += col_null_count\n",
    "\n",
    "print(f\"\\n‚úÖ Todos los valores faltantes han sido rellenados\")\n",
    "print(f\"Valores NULL restantes: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750527f6",
   "metadata": {},
   "source": [
    "### Paso 6: Capitalizar texto (Title Case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91cda248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Texto capitalizado (Title Case)\n"
     ]
    }
   ],
   "source": [
    "# Capitalizar texto en columnas de string (Title Case)\n",
    "# UDF para capitalizar texto\n",
    "def capitalize_text_udf(value):\n",
    "    \"\"\"Capitaliza la primera letra de cada palabra\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    # Convertir a string y capitalizar cada palabra\n",
    "    return str(value).title()\n",
    "\n",
    "capitalize_text = udf(capitalize_text_udf, StringType())\n",
    "\n",
    "# Aplicar capitalizaci√≥n a columnas de texto (excepto las que tienen valores \"X empty\")\n",
    "text_cols_to_capitalize = ['operation', 'district', 'neighborhood', 'condition', 'energy_certificate']\n",
    "\n",
    "for col_name in text_cols_to_capitalize:\n",
    "    if col_name in df_clean.columns:\n",
    "        # Solo capitalizar si no es un valor \"empty\"\n",
    "        df_clean = df_clean.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).contains(\" empty\"), col(col_name))\n",
    "            .otherwise(initcap(col(col_name)))  # initcap capitaliza la primera letra de cada palabra\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Texto capitalizado (Title Case)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71ca65",
   "metadata": {},
   "source": [
    "### Verificaci√≥n: Comparaci√≥n antes/despu√©s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a8ba5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EJEMPLOS DE LIMPIEZA ===\n",
      "\n",
      "ANTES (RAW):\n",
      "+----------+-----+---------+---------+------------+--------+-------------------+\n",
      "|surface_m2|rooms|bathrooms|price_eur|price_per_m2|elevator|district           |\n",
      "+----------+-----+---------+---------+------------+--------+-------------------+\n",
      "|89 m¬≤     |?    |2        |?        |4240 ‚Ç¨/m2   |Y       |Unknown            |\n",
      "|171       |N/A  |1        |?        |7920.91     |?       |Eixampl            |\n",
      "|?         |2+   |?        |317642 ‚Ç¨ |?           |Y       |Sant Mart√≠         |\n",
      "|N/A       |three|two      |N/A      |5484 ‚Ç¨/m2   |N       |SANTS              |\n",
      "|?         |2+   |?        |N/A      |?           |S√≠      |SANTS              |\n",
      "|127 m¬≤    |three|2        |491626 ‚Ç¨ |N/A         |Y       |Ciutat Vella       |\n",
      "|?         |2+   |two      |N/A      |?           |N       |Sants-Montju√Øc     |\n",
      "|?         |three|?        |1282371  |4093 ‚Ç¨/m2   |Y       |Sarri√†-Sant Gervasi|\n",
      "|127 m¬≤    |2+   |3        |?        |6630.1      |unknown |Les Corts          |\n",
      "|N/A       |2+   |N/A      |4512     |7856.74     |no      |Sant Andreu        |\n",
      "+----------+-----+---------+---------+------------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "DESPU√âS (CLEAN):\n",
      "+-----------------+-----+---------+-----------------+-----------------+--------+-------------------+\n",
      "|surface_m2       |rooms|bathrooms|price_eur        |price_per_m2     |elevator|district           |\n",
      "+-----------------+-----+---------+-----------------+-----------------+--------+-------------------+\n",
      "|89.0             |3    |2        |263348.9583646475|4240.0           |true    |Unknown            |\n",
      "|171.0            |3    |1        |263348.9583646475|792091.0         |false   |Eixampl            |\n",
      "|106.5811866378594|3    |1        |317642.0         |281963.1486186731|true    |Sant Mart√≠         |\n",
      "|106.5811866378594|3    |1        |263348.9583646475|5484.0           |false   |Sants              |\n",
      "|106.5811866378594|3    |1        |263348.9583646475|281963.1486186731|true    |Sants              |\n",
      "|127.0            |3    |2        |491626.0         |281963.1486186731|true    |Ciutat Vella       |\n",
      "|106.5811866378594|3    |1        |263348.9583646475|281963.1486186731|false   |Sants-montju√Øc     |\n",
      "|106.5811866378594|3    |1        |1282371.0        |4093.0           |true    |Sarri√†-sant Gervasi|\n",
      "|127.0            |3    |3        |263348.9583646475|66301.0          |false   |Les Corts          |\n",
      "|106.5811866378594|3    |1        |4512.0           |785674.0         |false   |Sant Andreu        |\n",
      "+-----------------+-----+---------+-----------------+-----------------+--------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Mostrar ejemplos de limpieza\n",
    "print(\"=== EJEMPLOS DE LIMPIEZA ===\\n\")\n",
    "print(\"ANTES (RAW):\")\n",
    "df_raw.select('surface_m2', 'rooms', 'bathrooms', 'price_eur', 'price_per_m2', 'elevator', 'district').show(10, truncate=False)\n",
    "print(\"\\nDESPU√âS (CLEAN):\")\n",
    "df_clean.select('surface_m2', 'rooms', 'bathrooms', 'price_eur', 'price_per_m2', 'elevator', 'district').show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da654627",
   "metadata": {},
   "source": [
    "### Resumen de la transformaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "582ca5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESUMEN DE LA TRANSFORMACI√ìN ===\n",
      "\n",
      "Filas: 10000\n",
      "Columnas: 20\n",
      "\n",
      "Esquema del dataset limpio:\n",
      "root\n",
      " |-- listing_id: string (nullable = false)\n",
      " |-- operation: string (nullable = false)\n",
      " |-- district: string (nullable = false)\n",
      " |-- neighborhood: string (nullable = false)\n",
      " |-- address: string (nullable = false)\n",
      " |-- surface_m2: double (nullable = false)\n",
      " |-- rooms: integer (nullable = false)\n",
      " |-- bathrooms: integer (nullable = false)\n",
      " |-- price_eur: double (nullable = false)\n",
      " |-- price_per_m2: double (nullable = false)\n",
      " |-- floor: string (nullable = false)\n",
      " |-- elevator: boolean (nullable = false)\n",
      " |-- balcony: boolean (nullable = false)\n",
      " |-- furnished: boolean (nullable = false)\n",
      " |-- condition: string (nullable = false)\n",
      " |-- energy_certificate: string (nullable = false)\n",
      " |-- has_parking: boolean (nullable = false)\n",
      " |-- latitude: double (nullable = false)\n",
      " |-- longitude: double (nullable = false)\n",
      " |-- agency: string (nullable = false)\n",
      "\n",
      "\n",
      "Primeras filas del dataset limpio:\n",
      "+----------------+---------+----------+---------------+-----------------+-----------------+-----+---------+-----------------+-----------------+------+--------+-------+---------+---------------+------------------------+-----------+------------------+-----------------+---------------+\n",
      "|listing_id      |operation|district  |neighborhood   |address          |surface_m2       |rooms|bathrooms|price_eur        |price_per_m2     |floor |elevator|balcony|furnished|condition      |energy_certificate      |has_parking|latitude          |longitude        |agency         |\n",
      "+----------------+---------+----------+---------------+-----------------+-----------------+-----+---------+-----------------+-----------------+------+--------+-------+---------+---------------+------------------------+-----------+------------------+-----------------+---------------+\n",
      "|ID_0            |Alquiler |Unknown   |Sagrada Fam√≠lia|C/ Arag√≥ 395     |89.0             |3    |2        |263348.9583646475|4240.0           |1¬∫    |true    |false  |false    |Average        |energy_certificate empty|false      |41.192376703741246|2.082139006101164|Particular     |\n",
      "|listing_id empty|Vender   |Eixampl   |Les Corts      |Passeig de Gr√†cia|171.0            |3    |1        |263348.9583646475|792091.0         |√°tico |false   |false  |false    |condition empty|D                       |false      |41.192376703741246|2.082139006101164|Housfy         |\n",
      "|ID_2            |Lease    |Sant Mart√≠|El Clot        |C/ Mallorca 316  |106.5811866378594|3    |1        |317642.0         |281963.1486186731|2¬∫    |true    |false  |false    |Average        |D                       |false      |41.3997           |2.082139006101164|Engel & V√∂lkers|\n",
      "|listing_id empty|Alquiler |Sants     |Sagrada Fam√≠lia|Calle Falsa 123  |106.5811866378594|3    |1        |263348.9583646475|5484.0           |s√≥tano|false   |true   |false    |A Reformar     |A                       |true       |41.192376703741246|2.0              |Engel & V√∂lkers|\n",
      "|5               |Buy      |Sants     |Les Corts      |C/ Gran Via 245  |106.5811866378594|3    |1        |263348.9583646475|281963.1486186731|4¬∫    |true    |false  |true     |Average        |F                       |true       |41.192376703741246|2.0              |Particular     |\n",
      "+----------------+---------+----------+---------------+-----------------+-----------------+-----+---------+-----------------+-----------------+------+--------+-------+---------+---------------+------------------------+-----------+------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=== RESUMEN DE LA TRANSFORMACI√ìN ===\\n\")\n",
    "print(f\"Filas: {df_clean.count()}\")\n",
    "print(f\"Columnas: {len(df_clean.columns)}\")\n",
    "print(f\"\\nEsquema del dataset limpio:\")\n",
    "df_clean.printSchema()\n",
    "print(f\"\\nPrimeras filas del dataset limpio:\")\n",
    "df_clean.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc789e",
   "metadata": {},
   "source": [
    "## LOAD: Guardar datos limpios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a798d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos limpios guardados en: ../data/housing-barcelona-clean-pyspark.csv\n",
      "\n",
      "Archivo guardado exitosamente con 10000 filas y 20 columnas\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataframe limpio como un solo archivo CSV\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "output_path = \"../data/housing-barcelona-clean-pyspark.csv\"\n",
    "temp_dir = \"../data/temp_pyspark_output\"\n",
    "\n",
    "# Eliminar el directorio de salida si existe (por si hay una ejecuci√≥n anterior)\n",
    "if os.path.exists(output_path):\n",
    "    if os.path.isdir(output_path):\n",
    "        shutil.rmtree(output_path)\n",
    "    else:\n",
    "        os.remove(output_path)\n",
    "\n",
    "# Eliminar directorio temporal si existe\n",
    "if os.path.exists(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "# Guardar en directorio temporal con una sola partici√≥n\n",
    "df_clean.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_dir)\n",
    "\n",
    "# Encontrar el archivo CSV generado (PySpark crea archivos como part-00000-*.csv)\n",
    "csv_files = glob.glob(os.path.join(temp_dir, \"part-*.csv\"))\n",
    "\n",
    "if csv_files:\n",
    "    # Leer el archivo CSV generado\n",
    "    df_pandas = pd.read_csv(csv_files[0])\n",
    "    \n",
    "    # Guardar como un solo archivo CSV\n",
    "    df_pandas.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Eliminar el directorio temporal y su contenido\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Datos limpios guardados en: {output_path}\")\n",
    "    print(f\"\\nArchivo guardado exitosamente con {len(df_pandas)} filas y {len(df_pandas.columns)} columnas\")\n",
    "else:\n",
    "    print(\"‚ùå Error: No se encontr√≥ el archivo CSV generado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c11fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAR EN SQLITE: Crear Datawarehouse en SQLite\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "print(\"=== CARGA EN SQLITE: CREANDO DATAWAREHOUSE ===\\n\")\n",
    "\n",
    "# Convertir DataFrame de PySpark a Pandas para guardar en SQLite\n",
    "print(\"üìä Convirtiendo DataFrame de PySpark a Pandas...\")\n",
    "df_pandas_clean = df_clean.toPandas()\n",
    "print(f\"‚úÖ DataFrame convertido: {len(df_pandas_clean)} filas\")\n",
    "\n",
    "# Ruta de la base de datos SQLite\n",
    "db_path = \"../warehouse/warehouse_pyspark.db\"\n",
    "\n",
    "# Crear conexi√≥n usando SQLAlchemy (requerido para to_sql)\n",
    "engine = create_engine(f'sqlite:///{db_path}', echo=False)\n",
    "\n",
    "# Preparar datos para tablas dimensionales\n",
    "print(\"\\nüìä Preparando datos para tablas dimensionales...\")\n",
    "\n",
    "# Tabla dim_district\n",
    "df_dim_district = pd.DataFrame({\n",
    "    'district_name': df_pandas_clean['district'].unique()\n",
    "}).dropna()\n",
    "df_dim_district = df_dim_district[df_dim_district['district_name'] != 'district empty']\n",
    "print(f\"‚úÖ dim_district: {len(df_dim_district)} distritos √∫nicos\")\n",
    "\n",
    "# Tabla dim_neighborhood\n",
    "df_dim_neighborhood = df_pandas_clean[['neighborhood', 'district']].drop_duplicates()\n",
    "df_dim_neighborhood = df_dim_neighborhood[\n",
    "    (df_dim_neighborhood['neighborhood'] != 'neighborhood empty') &\n",
    "    (df_dim_neighborhood['district'] != 'district empty')\n",
    "].rename(columns={'neighborhood': 'neighborhood_name', 'district': 'district_name'})\n",
    "print(f\"‚úÖ dim_neighborhood: {len(df_dim_neighborhood)} barrios √∫nicos\")\n",
    "\n",
    "# Tabla dim_operation\n",
    "df_dim_operation = pd.DataFrame({\n",
    "    'operation_type': df_pandas_clean['operation'].unique()\n",
    "}).dropna()\n",
    "df_dim_operation = df_dim_operation[df_dim_operation['operation_type'] != 'operation empty']\n",
    "print(f\"‚úÖ dim_operation: {len(df_dim_operation)} tipos de operaci√≥n √∫nicos\")\n",
    "\n",
    "# Tabla dim_agency\n",
    "df_dim_agency = pd.DataFrame({\n",
    "    'agency_name': df_pandas_clean['agency'].unique()\n",
    "}).dropna()\n",
    "df_dim_agency = df_dim_agency[df_dim_agency['agency_name'] != 'agency empty']\n",
    "print(f\"‚úÖ dim_agency: {len(df_dim_agency)} agencias √∫nicas\")\n",
    "\n",
    "# Tabla dim_condition\n",
    "df_dim_condition = pd.DataFrame({\n",
    "    'condition_type': df_pandas_clean['condition'].unique()\n",
    "}).dropna()\n",
    "df_dim_condition = df_dim_condition[df_dim_condition['condition_type'] != 'condition empty']\n",
    "print(f\"‚úÖ dim_condition: {len(df_dim_condition)} condiciones √∫nicas\")\n",
    "\n",
    "# Tabla dim_energy_certificate\n",
    "df_dim_energy_certificate = pd.DataFrame({\n",
    "    'certificate_type': df_pandas_clean['energy_certificate'].unique()\n",
    "}).dropna()\n",
    "df_dim_energy_certificate = df_dim_energy_certificate[\n",
    "    df_dim_energy_certificate['certificate_type'] != 'energy_certificate empty'\n",
    "]\n",
    "print(f\"‚úÖ dim_energy_certificate: {len(df_dim_energy_certificate)} certificados √∫nicos\")\n",
    "\n",
    "# Guardar tablas dimensionales en SQLite\n",
    "print(\"\\nüíæ Guardando tablas dimensionales en SQLite...\")\n",
    "df_dim_district.to_sql('dim_district', engine, if_exists='replace', index=False)\n",
    "df_dim_neighborhood.to_sql('dim_neighborhood', engine, if_exists='replace', index=False)\n",
    "df_dim_operation.to_sql('dim_operation', engine, if_exists='replace', index=False)\n",
    "df_dim_agency.to_sql('dim_agency', engine, if_exists='replace', index=False)\n",
    "df_dim_condition.to_sql('dim_condition', engine, if_exists='replace', index=False)\n",
    "df_dim_energy_certificate.to_sql('dim_energy_certificate', engine, if_exists='replace', index=False)\n",
    "print(\"‚úÖ Tablas dimensionales guardadas\")\n",
    "\n",
    "# Preparar tabla de hechos (fact_housing)\n",
    "print(\"\\nüìä Preparando tabla de hechos...\")\n",
    "df_fact_housing = df_pandas_clean.copy()\n",
    "print(f\"‚úÖ fact_housing: {len(df_fact_housing)} filas preparadas\")\n",
    "\n",
    "# Guardar tabla de hechos en SQLite\n",
    "print(\"\\nüíæ Guardando tabla de hechos en SQLite...\")\n",
    "df_fact_housing.to_sql('fact_housing', engine, if_exists='replace', index=False)\n",
    "print(\"‚úÖ Tabla de hechos guardada\")\n",
    "\n",
    "print(f\"\\n‚úÖ Datawarehouse creado exitosamente en: {db_path}\")\n",
    "print(f\"   ‚Ä¢ 1 tabla de hechos: fact_housing\")\n",
    "print(f\"   ‚Ä¢ 6 tablas de dimensiones: dim_district, dim_neighborhood, dim_operation, dim_agency, dim_condition, dim_energy_certificate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13542340",
   "metadata": {},
   "source": [
    "## CREACI√ìN DEL DATAWAREHOUSE\n",
    "\n",
    "### Generaci√≥n de DDLs para las tablas del Datawarehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5075e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Directorio ya existe: /app/warehouse\n",
      "üíæ Escribiendo archivo en: /app/warehouse/modelo_datawarehouse_pyspark.sql\n",
      "‚úÖ DDL del Datawarehouse generado exitosamente\n",
      "üìÑ Archivo guardado en: /app/warehouse/modelo_datawarehouse_pyspark.sql\n",
      "\n",
      "=== DDL GENERADO ===\n",
      "-- ============================================\n",
      "-- DDL para Datawarehouse - Housing Barcelona\n",
      "-- Generado desde ETL con PySpark\n",
      "-- ============================================\n",
      "\n",
      "-- Tabla de hechos: fact_housing\n",
      "CREATE TABLE IF NOT EXISTS fact_housing (\n",
      "    listing_id TEXT PRIMARY KEY,\n",
      "    operation_id INTEGER,\n",
      "    FOREIGN KEY (operation_id) REFERENCES dim_operation(operation_id),\n",
      "    district_id INTEGER,\n",
      "    FOREIGN KEY (district_id) REFERENCES dim_district(district_id),\n",
      "    neighborhood_id INTEGER,\n",
      "    FOREIGN KEY (neighborhood_id) REFERENCES dim_neighborhood(neighborhood_id),\n",
      "    address TEXT,\n",
      "    surface_m2 REAL,\n",
      "    rooms INTEGER,\n",
      "    bathrooms INTEGER,\n",
      "    price_eur REAL,\n",
      "    price_per_m2 REAL,\n",
      "    floor TEXT,\n",
      "    elevator BOOLEAN,\n",
      "    balcony BOOLEAN,\n",
      "    furnished BOOLEAN,\n",
      "    condition_id INTEGER,\n",
      "    FOREIGN KEY (condition_id) REFERENCES dim_condition(condition_id),\n",
      "    energy_certificate_id INTEGER,\n",
      "    FOREIGN KEY (energy_certificate_id) REFERENCES dim_energy_certificate(certificate_id),\n",
      "    has_parking BOOLEAN,\n",
      "    latitude REAL,\n",
      "    longitude REAL,\n",
      "    agency_id INTEGER,\n",
      "    FOREIGN KEY (agency_id) REFERENCES dim_agency(agency_id)\n",
      ");\n",
      "\n",
      "-- Tabla dimensional: dim_district\n",
      "CREATE TABLE IF NOT EXISTS dim_district (\n",
      "    district_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    district_name TEXT UNIQUE NOT NULL\n",
      ");\n",
      "\n",
      "-- Tabla dimensional: dim_neighborhood\n",
      "CREATE TABLE IF NOT EXISTS dim_neighborhood (\n",
      "    neighborhood_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    neighborhood_name TEXT UNIQUE NOT NULL,\n",
      "    district_id INTEGER,\n",
      "    FOREIGN KEY (district_id) REFERENCES dim_district(district_id)\n",
      ");\n",
      "\n",
      "-- Tabla dimensional: dim_operation\n",
      "CREATE TABLE IF NOT EXISTS dim_operation (\n",
      "    operation_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    operation_type TEXT UNIQUE NOT NULL\n",
      ");\n",
      "\n",
      "-- Tabla dimensional: dim_agency\n",
      "CREATE TABLE IF NOT EXISTS dim_agency (\n",
      "    agency_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    agency_name TEXT UNIQUE NOT NULL\n",
      ");\n",
      "\n",
      "-- Tabla dimensional: dim_condition\n",
      "CREATE TABLE IF NOT EXISTS dim_condition (\n",
      "    condition_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    condition_type TEXT UNIQUE NOT NULL\n",
      ");\n",
      "\n",
      "-- Tabla dimensional: dim_energy_certificate\n",
      "CREATE TABLE IF NOT EXISTS dim_energy_certificate (\n",
      "    certificate_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    certificate_type TEXT UNIQUE NOT NULL\n",
      ");\n",
      "\n",
      "-- √çndices para mejorar el rendimiento de consultas\n",
      "CREATE INDEX IF NOT EXISTS idx_fact_price ON fact_housing(price_eur);\n",
      "CREATE INDEX IF NOT EXISTS idx_fact_surface ON fact_housing(surface_m2);\n"
     ]
    }
   ],
   "source": [
    "# Generar DDL para el Datawarehouse basado en el esquema de PySpark\n",
    "def pyspark_dtype_to_sql(dtype):\n",
    "    \"\"\"Convierte tipos de datos de PySpark a tipos SQL\"\"\"\n",
    "    dtype_str = str(dtype).lower()\n",
    "    if 'int' in dtype_str:\n",
    "        return \"INTEGER\"\n",
    "    elif 'double' in dtype_str or 'float' in dtype_str:\n",
    "        return \"REAL\"\n",
    "    elif 'boolean' in dtype_str or 'bool' in dtype_str:\n",
    "        return \"BOOLEAN\"\n",
    "    elif 'timestamp' in dtype_str or 'date' in dtype_str:\n",
    "        return \"TIMESTAMP\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "\n",
    "# Crear DDL para la tabla principal\n",
    "ddl_statements = []\n",
    "ddl_statements.append(\"-- ============================================\")\n",
    "ddl_statements.append(\"-- DDL para Datawarehouse - Housing Barcelona\")\n",
    "ddl_statements.append(\"-- Generado desde ETL con PySpark\")\n",
    "ddl_statements.append(\"-- ============================================\\n\")\n",
    "\n",
    "ddl_statements.append(\"-- ============================================\\n\")\n",
    "ddl_statements.append(\"-- TABLA DE HECHOS\\n\")\n",
    "ddl_statements.append(\"-- ============================================\\n\")\n",
    "ddl_statements.append(\"-- ============================================\\n\")\n",
    "ddl_statements.append(\"-- TABLAS DIMENSIONALES\\n\")\n",
    "ddl_statements.append(\"-- ============================================\\n\")\n",
    "ddl_statements.append(\"-- Tabla dimensional: dim_district\")\n",
    "ddl_statements.append(\"CREATE TABLE IF NOT EXISTS dim_district (\")\n",
    "ddl_statements.append(\"    district_id INTEGER PRIMARY KEY AUTOINCREMENT,\")\n",
    "ddl_statements.append(\"    district_name TEXT UNIQUE NOT NULL\")\n",
    "ddl_statements.append(\");\\n\")\n",
    "\n",
    "ddl_statements.append(\"-- Tabla dimensional: dim_neighborhood\")\n",
    "ddl_statements.append(\"CREATE TABLE IF NOT EXISTS dim_neighborhood (\")\n",
    "ddl_statements.append(\"    neighborhood_id INTEGER PRIMARY KEY AUTOINCREMENT,\")\n",
    "ddl_statements.append(\"    neighborhood_name TEXT UNIQUE NOT NULL,\")\n",
    "ddl_statements.append(\"    district_id INTEGER,\")\n",
    "ddl_statements.append(\"    FOREIGN KEY (district_id) REFERENCES dim_district(district_id)\")\n",
    "ddl_statements.append(\");\\n\")\n",
    "\n",
    "ddl_statements.append(\"-- Tabla dimensional: dim_operation\")\n",
    "ddl_statements.append(\"CREATE TABLE IF NOT EXISTS dim_operation (\")\n",
    "ddl_statements.append(\"    operation_id INTEGER PRIMARY KEY AUTOINCREMENT,\")\n",
    "ddl_statements.append(\"    operation_type TEXT UNIQUE NOT NULL\")\n",
    "ddl_statements.append(\");\\n\")\n",
    "\n",
    "ddl_statements.append(\"-- Tabla dimensional: dim_agency\")\n",
    "ddl_statements.append(\"CREATE TABLE IF NOT EXISTS dim_agency (\")\n",
    "ddl_statements.append(\"    agency_id INTEGER PRIMARY KEY AUTOINCREMENT,\")\n",
    "ddl_statements.append(\"    agency_name TEXT UNIQUE NOT NULL\")\n",
    "ddl_statements.append(\");\\n\")\n",
    "\n",
    "ddl_statements.append(\"-- Tabla dimensional: dim_condition\")\n",
    "ddl_statements.append(\"CREATE TABLE IF NOT EXISTS dim_condition (\")\n",
    "ddl_statements.append(\"    condition_id INTEGER PRIMARY KEY AUTOINCREMENT,\")\n",
    "ddl_statements.append(\"    condition_type TEXT UNIQUE NOT NULL\")\n",
    "ddl_statements.append(\");\\n\")\n",
    "\n",
    "ddl_statements.append(\"-- Tabla dimensional: dim_energy_certificate\")\n",
    "ddl_statements.append(\"CREATE TABLE IF NOT EXISTS dim_energy_certificate (\")\n",
    "ddl_statements.append(\"    certificate_id INTEGER PRIMARY KEY AUTOINCREMENT,\")\n",
    "ddl_statements.append(\"    certificate_type TEXT UNIQUE NOT NULL\")\n",
    "ddl_statements.append(\");\\n\")\n",
    "\n",
    "# Crear √≠ndices para mejorar el rendimiento\n",
    "ddl_statements.append(\"-- √çndices para mejorar el rendimiento de consultas\")\n",
    "ddl_statements.append(\"CREATE INDEX IF NOT EXISTS idx_fact_price ON fact_housing(price_eur);\")\n",
    "ddl_statements.append(\"CREATE INDEX IF NOT EXISTS idx_fact_surface ON fact_housing(surface_m2);\")\n",
    "\n",
    "# Unir todas las declaraciones\n",
    "ddl_sql = \"\\n\".join(ddl_statements)\n",
    "\n",
    "# Guardar DDL en archivo\n",
    "import os\n",
    "import builtins\n",
    "import stat\n",
    "\n",
    "# Usar ruta relativa desde el notebook (que est√° en notebooks/)\n",
    "ddl_file_path = \"../warehouse/modelo_datawarehouse_pyspark.sql\"\n",
    "\n",
    "# Obtener la ruta absoluta y normalizarla\n",
    "ddl_file_abs = os.path.abspath(ddl_file_path)\n",
    "warehouse_dir = os.path.dirname(ddl_file_abs)\n",
    "\n",
    "# Verificar y crear el directorio warehouse de forma robusta\n",
    "success = False\n",
    "try:\n",
    "    # Verificar si el directorio existe\n",
    "    if not os.path.exists(warehouse_dir):\n",
    "        print(f\"üìÅ Creando directorio: {warehouse_dir}\")\n",
    "        # Crear el directorio y todos los padres necesarios\n",
    "        os.makedirs(warehouse_dir, exist_ok=True, mode=0o777)\n",
    "        print(f\"‚úÖ Directorio creado exitosamente\")\n",
    "    else:\n",
    "        print(f\"üìÅ Directorio ya existe: {warehouse_dir}\")\n",
    "    \n",
    "    # Verificar permisos de escritura\n",
    "    if not os.access(warehouse_dir, os.W_OK):\n",
    "        print(f\"‚ö†Ô∏è No hay permisos de escritura en: {warehouse_dir}\")\n",
    "        # Intentar cambiar permisos\n",
    "        try:\n",
    "            os.chmod(warehouse_dir, 0o777)\n",
    "            print(f\"‚úÖ Permisos actualizados\")\n",
    "        except Exception as perm_error:\n",
    "            print(f\"‚ö†Ô∏è No se pudieron cambiar permisos: {perm_error}\")\n",
    "    \n",
    "    # Intentar escribir el archivo\n",
    "    print(f\"üíæ Escribiendo archivo en: {ddl_file_abs}\")\n",
    "    with builtins.open(ddl_file_abs, 'w', encoding='utf-8') as f:\n",
    "        f.write(ddl_sql)\n",
    "    success = True\n",
    "    print(\"‚úÖ DDL del Datawarehouse generado exitosamente\")\n",
    "    print(f\"üìÑ Archivo guardado en: {ddl_file_abs}\\n\")\n",
    "    \n",
    "except (FileNotFoundError, PermissionError, OSError) as e:\n",
    "    print(f\"‚ùå Error al escribir en {ddl_file_abs}\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"   Directorio existe: {os.path.exists(warehouse_dir) if warehouse_dir else 'N/A'}\")\n",
    "    print(f\"   Permisos de escritura: {os.access(warehouse_dir, os.W_OK) if os.path.exists(warehouse_dir) else 'N/A'}\")\n",
    "    print(f\"\\nüí° El directorio warehouse/ puede no estar montado correctamente en Docker.\")\n",
    "    print(f\"   Verifica la configuraci√≥n del volumen en docker-compose.yml\")\n",
    "\n",
    "if success:\n",
    "    print(\"=== DDL GENERADO ===\")\n",
    "    print(ddl_sql)\n",
    "ddl_statements.append(\"-- Tabla de hechos: fact_housing\")\n",
    "ddl_statements.append(\"CREATE TABLE IF NOT EXISTS fact_housing (\")\n",
    "ddl_statements.append(\"    listing_id TEXT PRIMARY KEY,\")\n",
    "ddl_statements.append(\"    operation_id INTEGER,\")\n",
    "ddl_statements.append(\"    FOREIGN KEY (operation_id) REFERENCES dim_operation(operation_id),\")\n",
    "ddl_statements.append(\"    district_id INTEGER,\")\n",
    "ddl_statements.append(\"    FOREIGN KEY (district_id) REFERENCES dim_district(district_id),\")\n",
    "ddl_statements.append(\"    neighborhood_id INTEGER,\")\n",
    "ddl_statements.append(\"    FOREIGN KEY (neighborhood_id) REFERENCES dim_neighborhood(neighborhood_id),\")\n",
    "ddl_statements.append(\"    address TEXT,\")\n",
    "ddl_statements.append(\"    surface_m2 REAL,\")\n",
    "ddl_statements.append(\"    rooms INTEGER,\")\n",
    "ddl_statements.append(\"    bathrooms INTEGER,\")\n",
    "ddl_statements.append(\"    price_eur REAL,\")\n",
    "ddl_statements.append(\"    price_per_m2 REAL,\")\n",
    "ddl_statements.append(\"    floor TEXT,\")\n",
    "ddl_statements.append(\"    elevator BOOLEAN,\")\n",
    "ddl_statements.append(\"    balcony BOOLEAN,\")\n",
    "ddl_statements.append(\"    furnished BOOLEAN,\")\n",
    "ddl_statements.append(\"    condition_id INTEGER,\")\n",
    "ddl_statements.append(\"    FOREIGN KEY (condition_id) REFERENCES dim_condition(condition_id),\")\n",
    "ddl_statements.append(\"    energy_certificate_id INTEGER,\")\n",
    "ddl_statements.append(\"    FOREIGN KEY (energy_certificate_id) REFERENCES dim_energy_certificate(certificate_id),\")\n",
    "ddl_statements.append(\"    has_parking BOOLEAN,\")\n",
    "ddl_statements.append(\"    latitude REAL,\")\n",
    "ddl_statements.append(\"    longitude REAL,\")\n",
    "ddl_statements.append(\"    agency_id INTEGER,\")\n",
    "ddl_statements.append(\"    FOREIGN KEY (agency_id) REFERENCES dim_agency(agency_id)\")\n",
    "ddl_statements.append(\");\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
