# Proyecto RA1 - Big Data: ETL y Datawarehouse para Viviendas en Barcelona

**Autor:** Teo Aranda P√°ez  
**Fecha:** Diciembre 2024

---

## üìã Descripci√≥n del Proyecto

Este proyecto implementa un proceso completo de ETL (Extract, Transform, Load) para limpiar y transformar datos de viviendas en Barcelona, creando un Datawarehouse estructurado con modelo dimensional (Star Schema). El proyecto utiliza tanto **Pandas** como **PySpark** para procesar los datos, demostrando diferentes enfoques de procesamiento de datos.

### Objetivos

- Realizar exploraci√≥n y limpieza exhaustiva de datos de viviendas
- Implementar procesos ETL con Pandas y PySpark
- Crear un Datawarehouse con modelo dimensional (1 tabla de hechos + 6 tablas de dimensiones)
- Generar DDLs para la estructura del Datawarehouse
- Documentar todo el proceso y resultados

---

## üóÇÔ∏è Estructura de Carpetas

```
proyecto_ra1_teoarandapaez/
‚îú‚îÄ‚îÄ data/                          # Datos del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ housing-barcelona.csv      # Dataset original
‚îÇ   ‚îú‚îÄ‚îÄ housing-barcelona-clean.csv           # Dataset limpio (Pandas)
‚îÇ   ‚îî‚îÄ‚îÄ housing-barcelona-clean-pyspark.csv  # Dataset limpio (PySpark)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                     # Notebooks Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ 01_pandas.ipynb           # ETL con Pandas
‚îÇ   ‚îî‚îÄ‚îÄ 02_pyspark.ipynb          # ETL con PySpark
‚îÇ
‚îú‚îÄ‚îÄ warehouse/                     # Datawarehouse
‚îÇ   ‚îú‚îÄ‚îÄ warehouse_pandas.db       # Base de datos SQLite (Pandas)
‚îÇ   ‚îú‚îÄ‚îÄ warehouse_pyspark.db     # Base de datos SQLite (PySpark)
‚îÇ   ‚îú‚îÄ‚îÄ modelo_datawarehouse_pandas.sql    # DDL del modelo (Pandas)
‚îÇ   ‚îî‚îÄ‚îÄ modelo_datawarehouse_pyspark.sql   # DDL del modelo (PySpark)
‚îÇ
‚îú‚îÄ‚îÄ docker/                        # Configuraci√≥n Docker
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Imagen Docker
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml        # Orquestaci√≥n de contenedores
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt          # Dependencias Python
‚îÇ   ‚îî‚îÄ‚îÄ start-jupyter.sh         # Script de inicio Jupyter
‚îÇ
‚îî‚îÄ‚îÄ docs/                         # Documentaci√≥n
    ‚îú‚îÄ‚îÄ README.md                 # Este archivo
    ‚îî‚îÄ‚îÄ diagrama.drawio           # Diagrama del modelo dimensional
```

---

## üõ†Ô∏è Herramientas Utilizadas

### Lenguajes y Frameworks
- **Python 3.11**: Lenguaje de programaci√≥n principal
- **Pandas**: Biblioteca para manipulaci√≥n y an√°lisis de datos
- **PySpark 4.0.1**: Framework para procesamiento distribuido de datos
- **SQLite**: Base de datos relacional para el Datawarehouse
- **SQLAlchemy**: ORM para conexi√≥n con SQLite

### Herramientas de Desarrollo
- **Jupyter Notebook/Lab**: Entorno de desarrollo interactivo
- **Docker**: Contenedorizaci√≥n del entorno de desarrollo
- **Docker Compose**: Orquestaci√≥n de contenedores
- **draw.io**: Creaci√≥n de diagramas

### Librer√≠as Python Principales
- `pandas`: Manipulaci√≥n de DataFrames
- `pyspark`: Procesamiento distribuido
- `numpy`: Operaciones num√©ricas
- `sqlalchemy`: Conexi√≥n a bases de datos
- `sqlite3`: Interfaz para SQLite
- `re`: Expresiones regulares para limpieza de datos

---

## üìä Explicaci√≥n de Cada Fase

### Fase 1: Exploraci√≥n y Limpieza con Pandas (`01_pandas.ipynb`)

**Objetivo:** Explorar y limpiar el dataset usando Pandas.

**Proceso:**
1. **Carga de datos**: Lectura del CSV original (`housing-barcelona.csv`)
2. **An√°lisis exploratorio**: 
   - An√°lisis de tipos de datos
   - Detecci√≥n de valores faltantes y duplicados
   - Identificaci√≥n de valores problem√°ticos
3. **Limpieza de datos**:
   - Eliminaci√≥n de espacios (strip)
   - Conversi√≥n de valores vac√≠os a NaN
   - Transformaci√≥n de tipos de datos
   - Relleno de valores faltantes (strings con "X empty", n√∫meros con medias)

**Resultado:** Dataset limpio guardado en `housing-barcelona-clean.csv`

---

### Fase 2: Procesamiento con PySpark (`02_pyspark.ipynb`)

**Objetivo:** Replicar el proceso ETL usando PySpark para procesamiento distribuido.

**Proceso:**
1. **Creaci√≥n de SparkSession**: Inicializaci√≥n del contexto Spark
2. **Carga de datos**: Lectura del CSV con `spark.read.csv()`
3. **Transformaciones**:
   - **Filtrado**: `.filter()` para identificar valores problem√°ticos
   - **Selecci√≥n**: `.select()` para trabajar con columnas espec√≠ficas
   - **Nuevas columnas**: `.withColumn()` para transformaciones (trim, cast, UDFs)
   - **Agregaciones**: `.agg()` para calcular medias y estad√≠sticas
4. **Limpieza**: Similar a Pandas pero usando funciones de PySpark

**Resultado:** Dataset limpio guardado en `housing-barcelona-clean-pyspark.csv`

---

### Fase 3: Proceso ETL Completo con Pandas

**Objetivo:** Implementar el ciclo completo ETL y cargar en SQLite.

**Proceso:**

1. **EXTRACT (Extracci√≥n)**:
   ```python
   df_raw = pd.read_csv("../data/housing-barcelona.csv")
   ```

2. **TRANSFORM (Transformaci√≥n)**:
   - Limpieza de espacios
   - Conversi√≥n de tipos
   - Relleno de valores faltantes
   - Normalizaci√≥n de datos

3. **LOAD (Carga)**:
   - Creaci√≥n de tablas dimensionales desde los datos limpios
   - Guardado en SQLite usando `to_sql()`:
     ```python
     df_dim_district.to_sql('dim_district', engine, if_exists='replace', index=False)
     df_fact_housing.to_sql('fact_housing', engine, if_exists='replace', index=False)
     ```

**Resultado:** Base de datos SQLite `warehouse_pandas.db` con todas las tablas del Datawarehouse.

---

### Fase 4: Proceso ETL Completo con PySpark

**Objetivo:** Implementar el ciclo completo ETL con PySpark y cargar en SQLite.

**Proceso:**

1. **EXTRACT (Extracci√≥n)**:
   ```python
   df_raw = spark.read.csv("../data/housing-barcelona.csv", header=True)
   ```

2. **TRANSFORM (Transformaci√≥n)**:
   - Filtrado con `.filter()`
   - Agrupaciones con `.groupBy()` y `.agg()`
   - Transformaciones con `.withColumn()` y UDFs
   - Conversi√≥n de tipos con `.cast()`

3. **LOAD (Carga)**:
   - Conversi√≥n a Pandas: `df_pandas_clean = df_clean.toPandas()`
   - Guardado en SQLite usando `to_sql()`:
     ```python
     df_fact_housing.to_sql('fact_housing', engine, if_exists='replace', index=False)
     ```

**Resultado:** Base de datos SQLite `warehouse_pyspark.db` con todas las tablas del Datawarehouse.

---

### Fase 5: Modelo de Data Warehouse

**Objetivo:** Definir la estructura del Datawarehouse con DDLs.

**Archivos generados:**
- `warehouse/modelo_datawarehouse_pandas.sql`
- `warehouse/modelo_datawarehouse_pyspark.sql`

**Estructura del modelo:**

#### Tabla de Hechos
- **`fact_housing`**: Contiene todas las m√©tricas y medidas de las viviendas
  - Clave primaria: `listing_id`
  - Claves for√°neas hacia todas las dimensiones
  - Medidas: `surface_m2`, `rooms`, `bathrooms`, `price_eur`, `price_per_m2`, `latitude`, `longitude`
  - Atributos descriptivos: `address`, `floor`, `elevator`, `balcony`, `furnished`, `has_parking`

#### Tablas Dimensionales (6 tablas)
1. **`dim_district`**: Distritos de Barcelona
2. **`dim_neighborhood`**: Barrios (con relaci√≥n a distritos)
3. **`dim_operation`**: Tipos de operaci√≥n (alquiler, venta, etc.)
4. **`dim_agency`**: Agencias inmobiliarias
5. **`dim_condition`**: Condiciones de la vivienda
6. **`dim_energy_certificate`**: Certificados energ√©ticos

**Relaciones:**
- `fact_housing` ‚Üí `dim_operation` (operation_id)
- `fact_housing` ‚Üí `dim_district` (district_id)
- `fact_housing` ‚Üí `dim_neighborhood` (neighborhood_id)
- `fact_housing` ‚Üí `dim_agency` (agency_id)
- `fact_housing` ‚Üí `dim_condition` (condition_id)
- `fact_housing` ‚Üí `dim_energy_certificate` (energy_certificate_id)
- `dim_neighborhood` ‚Üí `dim_district` (district_id)

---

### Fase 6: Docker

**Objetivo:** Contenedorizar el entorno de desarrollo.

**Archivos en `docker/`:**
- **`Dockerfile`**: Define la imagen con Python, Jupyter, Pandas, PySpark y SQLAlchemy
- **`docker-compose.yml`**: Orquesta el contenedor con vol√∫menes montados
- **`requirements.txt`**: Dependencias de Python
- **`start-jupyter.sh`**: Script de inicio de Jupyter sin autenticaci√≥n

**Caracter√≠sticas:**
- Base: `python:3.11-slim`
- Instalaci√≥n de librer√≠as Python: pandas, pyspark, jupyter, sqlalchemy
- Java JDK para PySpark
- Vol√∫menes montados: notebooks, data, warehouse
- Puerto 8888 expuesto para Jupyter

---

### Fase 7: Documentaci√≥n

**Objetivo:** Documentar todo el proyecto.

**Contenido:**
- Este README.md con toda la informaci√≥n del proyecto
- Diagrama del modelo dimensional en `diagrama.drawio`
- Explicaci√≥n de cada fase del proceso
- Instrucciones de ejecuci√≥n
- Consultas SQL de ejemplo

---

## üöÄ Instrucciones de Ejecuci√≥n

### Opci√≥n 1: Con Docker (Recomendado)

1. **Navegar al directorio docker**:
   ```bash
   cd docker
   ```

2. **Construir y ejecutar el contenedor**:
   ```bash
   docker-compose up --build
   ```

3. **Acceder a Jupyter**:
   - Abrir navegador en: `http://localhost:8888`
   - Los notebooks estar√°n disponibles en `/app/notebooks`

4. **Ejecutar los notebooks**:
   - Abrir `01_pandas.ipynb` y ejecutar todas las celdas
   - Abrir `02_pyspark.ipynb` y ejecutar todas las celdas

### Opci√≥n 2: Sin Docker (Instalaci√≥n Local)

1. **Instalar dependencias**:
   ```bash
   pip install jupyter pandas pyspark sqlalchemy
   ```

2. **Instalar Java JDK** (requerido para PySpark):
   ```bash
   # macOS
   brew install openjdk
   
   # Linux
   sudo apt-get install default-jdk
   ```

3. **Configurar variables de entorno**:
   ```bash
   export JAVA_HOME=/usr/lib/jvm/default-java
   export PATH=$PATH:$JAVA_HOME/bin
   ```

4. **Iniciar Jupyter**:
   ```bash
   jupyter notebook
   ```

5. **Ejecutar los notebooks**:
   - Abrir `notebooks/01_pandas.ipynb`
   - Abrir `notebooks/02_pyspark.ipynb`

---

## üîÑ Explicaci√≥n Breve de Cada ETL

### ETL con Pandas (`01_pandas.ipynb`)

**Ventajas:**
- Sintaxis simple e intuitiva
- Ideal para datasets que caben en memoria
- Procesamiento r√°pido para datos peque√±os/medianos

**Proceso:**
1. **Extracci√≥n**: `pd.read_csv()` carga el dataset completo en memoria
2. **Transformaci√≥n**: 
   - Operaciones vectorizadas de Pandas (muy eficientes)
   - Uso de `.str.strip()`, `.fillna()`, `.astype()`
   - Funciones personalizadas con `apply()`
3. **Carga**: 
   - Guardado directo en SQLite con `to_sql()`
   - Creaci√≥n autom√°tica de tablas

**Resultado:** Dataset limpio y Datawarehouse en `warehouse_pandas.db`

---

### ETL con PySpark (`02_pyspark.ipynb`)

**Ventajas:**
- Procesamiento distribuido (escalable)
- Ideal para datasets grandes
- Optimizaci√≥n autom√°tica de consultas

**Proceso:**
1. **Extracci√≥n**: `spark.read.csv()` carga datos de forma distribuida
2. **Transformaci√≥n**:
   - Operaciones lazy evaluation (se ejecutan cuando es necesario)
   - UDFs (User Defined Functions) para transformaciones complejas
   - Transformaciones inmutables con `.withColumn()`
   - Agregaciones con `.agg()` y `.groupBy()`
3. **Carga**:
   - Conversi√≥n a Pandas: `toPandas()` (para datasets peque√±os)
   - Guardado en SQLite con `to_sql()`

**Resultado:** Dataset limpio y Datawarehouse en `warehouse_pyspark.db`

**Diferencias clave:**
- PySpark usa evaluaci√≥n diferida (lazy evaluation)
- PySpark es inmutable (cada transformaci√≥n crea un nuevo DataFrame)
- PySpark puede procesar datos m√°s grandes que la memoria disponible

---

## üíæ C√≥mo se Cargaron los Datos en SQLite

### Proceso de Carga

#### 1. Preparaci√≥n de Tablas Dimensionales

**Para Pandas:**
```python
# Extraer valores √∫nicos para cada dimensi√≥n
df_dim_district = pd.DataFrame({
    'district_name': df_clean['district'].unique()
}).dropna()

# Filtrar valores "empty"
df_dim_district = df_dim_district[
    df_dim_district['district_name'] != 'district empty'
]
```

**Para PySpark:**
```python
# Convertir a Pandas primero
df_pandas_clean = df_clean.toPandas()

# Luego crear dimensiones igual que en Pandas
df_dim_district = pd.DataFrame({
    'district_name': df_pandas_clean['district'].unique()
}).dropna()
```

#### 2. Creaci√≥n de Conexi√≥n SQLite

```python
from sqlalchemy import create_engine

db_path = "../warehouse/warehouse_pandas.db"  # o warehouse_pyspark.db
engine = create_engine(f'sqlite:///{db_path}', echo=False)
```

#### 3. Guardado de Tablas

```python
# Guardar tablas dimensionales
df_dim_district.to_sql('dim_district', engine, if_exists='replace', index=False)
df_dim_neighborhood.to_sql('dim_neighborhood', engine, if_exists='replace', index=False)
df_dim_operation.to_sql('dim_operation', engine, if_exists='replace', index=False)
df_dim_agency.to_sql('dim_agency', engine, if_exists='replace', index=False)
df_dim_condition.to_sql('dim_condition', engine, if_exists='replace', index=False)
df_dim_energy_certificate.to_sql('dim_energy_certificate', engine, if_exists='replace', index=False)

# Guardar tabla de hechos
df_fact_housing = df_clean.copy()  # o df_pandas_clean.copy()
df_fact_housing.to_sql('fact_housing', engine, if_exists='replace', index=False)
```

#### 4. Estructura Final en SQLite

Cada base de datos SQLite contiene:
- **1 tabla de hechos**: `fact_housing` (10,000 filas)
- **6 tablas dimensionales**: 
  - `dim_district` (~14 distritos √∫nicos)
  - `dim_neighborhood` (~14 barrios √∫nicos)
  - `dim_operation` (~7 tipos de operaci√≥n)
  - `dim_agency` (~7 agencias)
  - `dim_condition` (~7 condiciones)
  - `dim_energy_certificate` (~8 certificados)

---

## üìê Diagrama del Modelo Dimensional

El diagrama del modelo dimensional est√° disponible en `docs/diagrama.drawio`. Este diagrama muestra:

- **Modelo Star Schema**: Una tabla de hechos central (`fact_housing`) rodeada de tablas dimensionales
- **Relaciones**: Claves for√°neas desde la tabla de hechos hacia las dimensiones
- **Cardinalidades**: Relaciones uno-a-muchos entre hechos y dimensiones

### Estructura Visual:

```
                    fact_housing
                         |
        +----------------+----------------+
        |                |                |
   dim_district    dim_neighborhood   dim_operation
        |                |                |
   dim_agency      dim_condition   dim_energy_certificate
```

**Nota:** El diagrama completo con todas las relaciones y campos est√° disponible en `docs/diagrama.drawio`. Puede abrirse con [draw.io](https://app.diagrams.net/) o cualquier editor compatible.

---

## üîç Consultas y Queries que se Pueden Realizar

### Consultas B√°sicas

#### 1. Obtener todas las viviendas con sus dimensiones
```sql
SELECT 
    f.listing_id,
    f.price_eur,
    f.surface_m2,
    f.rooms,
    d.district_name,
    n.neighborhood_name,
    o.operation_type,
    a.agency_name
FROM fact_housing f
JOIN dim_district d ON f.district_id = d.district_id
JOIN dim_neighborhood n ON f.neighborhood_id = n.neighborhood_id
JOIN dim_operation o ON f.operation_id = o.operation_id
JOIN dim_agency a ON f.agency_id = a.agency_id
LIMIT 10;
```

#### 2. Precio promedio por distrito
```sql
SELECT 
    d.district_name,
    AVG(f.price_eur) as precio_promedio,
    COUNT(*) as total_viviendas
FROM fact_housing f
JOIN dim_district d ON f.district_id = d.district_id
GROUP BY d.district_name
ORDER BY precio_promedio DESC;
```

#### 3. Viviendas por tipo de operaci√≥n
```sql
SELECT 
    o.operation_type,
    COUNT(*) as cantidad,
    AVG(f.price_eur) as precio_promedio
FROM fact_housing f
JOIN dim_operation o ON f.operation_id = o.operation_id
GROUP BY o.operation_type;
```

#### 4. Distribuci√≥n por certificado energ√©tico
```sql
SELECT 
    e.certificate_type,
    COUNT(*) as cantidad,
    AVG(f.price_eur) as precio_promedio,
    AVG(f.surface_m2) as superficie_promedio
FROM fact_housing f
JOIN dim_energy_certificate e ON f.energy_certificate_id = e.certificate_id
GROUP BY e.certificate_type
ORDER BY cantidad DESC;
```

#### 5. Top 10 barrios m√°s caros
```sql
SELECT 
    n.neighborhood_name,
    d.district_name,
    AVG(f.price_eur) as precio_promedio,
    AVG(f.price_per_m2) as precio_m2_promedio,
    COUNT(*) as total_viviendas
FROM fact_housing f
JOIN dim_neighborhood n ON f.neighborhood_id = n.neighborhood_id
JOIN dim_district d ON n.district_id = d.district_id
GROUP BY n.neighborhood_name, d.district_name
ORDER BY precio_promedio DESC
LIMIT 10;
```

#### 6. Viviendas con caracter√≠sticas espec√≠ficas
```sql
SELECT 
    f.listing_id,
    f.price_eur,
    f.surface_m2,
    f.rooms,
    f.bathrooms,
    f.elevator,
    f.balcony,
    f.has_parking,
    c.condition_type,
    e.certificate_type
FROM fact_housing f
JOIN dim_condition c ON f.condition_id = c.condition_id
JOIN dim_energy_certificate e ON f.energy_certificate_id = e.certificate_id
WHERE f.elevator = 1 
  AND f.balcony = 1 
  AND f.has_parking = 1
  AND f.rooms >= 3
ORDER BY f.price_eur;
```

#### 7. An√°lisis por agencia inmobiliaria
```sql
SELECT 
    a.agency_name,
    COUNT(*) as total_propiedades,
    AVG(f.price_eur) as precio_promedio,
    MIN(f.price_eur) as precio_minimo,
    MAX(f.price_eur) as precio_maximo,
    AVG(f.surface_m2) as superficie_promedio
FROM fact_housing f
JOIN dim_agency a ON f.agency_id = a.agency_id
GROUP BY a.agency_name
ORDER BY total_propiedades DESC;
```

#### 8. Relaci√≥n precio/superficie por condici√≥n
```sql
SELECT 
    c.condition_type,
    AVG(f.price_per_m2) as precio_m2_promedio,
    AVG(f.surface_m2) as superficie_promedio,
    COUNT(*) as cantidad
FROM fact_housing f
JOIN dim_condition c ON f.condition_id = c.condition_id
WHERE f.price_per_m2 IS NOT NULL
GROUP BY c.condition_type
ORDER BY precio_m2_promedio DESC;
```

### Consultas Avanzadas

#### 9. An√°lisis comparativo por distrito y operaci√≥n
```sql
SELECT 
    d.district_name,
    o.operation_type,
    COUNT(*) as cantidad,
    AVG(f.price_eur) as precio_promedio,
    AVG(f.surface_m2) as superficie_promedio,
    AVG(f.rooms) as habitaciones_promedio
FROM fact_housing f
JOIN dim_district d ON f.district_id = d.district_id
JOIN dim_operation o ON f.operation_id = o.operation_id
GROUP BY d.district_name, o.operation_type
ORDER BY d.district_name, precio_promedio DESC;
```

#### 10. Viviendas m√°s econ√≥micas por m¬≤ en cada distrito
```sql
SELECT 
    d.district_name,
    f.listing_id,
    f.price_eur,
    f.surface_m2,
    f.price_per_m2,
    n.neighborhood_name
FROM fact_housing f
JOIN dim_district d ON f.district_id = d.district_id
JOIN dim_neighborhood n ON f.neighborhood_id = n.neighborhood_id
WHERE f.price_per_m2 IS NOT NULL
  AND f.price_per_m2 = (
    SELECT MIN(f2.price_per_m2)
    FROM fact_housing f2
    WHERE f2.district_id = f.district_id
      AND f2.price_per_m2 IS NOT NULL
  )
ORDER BY d.district_name;
```

---

## üìä Ejecutar Consultas en SQLite

### Desde Python

```python
import sqlite3
import pandas as pd

# Conectar a la base de datos
conn = sqlite3.connect('../warehouse/warehouse_pandas.db')

# Ejecutar consulta
query = """
SELECT 
    d.district_name,
    AVG(f.price_eur) as precio_promedio,
    COUNT(*) as total_viviendas
FROM fact_housing f
JOIN dim_district d ON f.district_id = d.district_id
GROUP BY d.district_name
ORDER BY precio_promedio DESC;
"""

df_result = pd.read_sql_query(query, conn)
print(df_result)

conn.close()
```

### Desde l√≠nea de comandos

```bash
# Abrir SQLite
sqlite3 warehouse/warehouse_pandas.db

# Ejecutar consulta
SELECT district_name, COUNT(*) 
FROM dim_district 
GROUP BY district_name;
```

---

## üéì Conclusiones y Aprendizajes

### Conclusiones T√©cnicas

1. **Pandas vs PySpark**:
   - **Pandas** es m√°s r√°pido para datasets peque√±os/medianos que caben en memoria
   - **PySpark** es mejor para datasets grandes y procesamiento distribuido
   - La sintaxis de Pandas es m√°s intuitiva, pero PySpark ofrece m√°s escalabilidad

2. **Limpieza de Datos**:
   - La detecci√≥n de valores problem√°ticos es crucial antes de la transformaci√≥n
   - El rellenado de valores faltantes debe hacerse seg√∫n el tipo de dato
   - La normalizaci√≥n de tipos es esencial para an√°lisis posteriores

3. **Modelo Dimensional**:
   - El modelo Star Schema facilita las consultas anal√≠ticas
   - Las claves for√°neas aseguran la integridad referencial
   - Las tablas dimensionales permiten an√°lisis multidimensionales eficientes

4. **SQLite como Datawarehouse**:
   - SQLite es adecuado para proyectos peque√±os/medianos
   - `to_sql()` de Pandas facilita la carga de datos
   - Las consultas SQL son eficientes para an√°lisis

### Aprendizajes del Proyecto

1. **ETL Completo**:
   - Entendimiento profundo del proceso ETL completo
   - Diferencia entre procesamiento en memoria (Pandas) y distribuido (PySpark)
   - Importancia de la validaci√≥n de datos en cada etapa

2. **Datawarehouse**:
   - Dise√±o de modelos dimensionales (Star Schema)
   - Creaci√≥n de DDLs para estructuras de base de datos
   - Relaciones entre tablas de hechos y dimensiones

3. **Docker**:
   - Contenedorizaci√≥n de entornos de desarrollo
   - Configuraci√≥n de vol√∫menes para persistencia de datos
   - Simplificaci√≥n del despliegue y colaboraci√≥n

4. **Documentaci√≥n**:
   - Importancia de documentar cada paso del proceso
   - Creaci√≥n de diagramas para visualizar estructuras
   - Documentaci√≥n de consultas y casos de uso

### Desaf√≠os Encontrados y Soluciones

1. **Problema**: Valores faltantes en diferentes formatos (`?`, `N/A`, `NULL`, etc.)
   - **Soluci√≥n**: Normalizaci√≥n de todos los valores vac√≠os a `NaN` antes del procesamiento

2. **Problema**: Tipos de datos inconsistentes (n√∫meros como strings, texto como n√∫meros)
   - **Soluci√≥n**: Funciones personalizadas para extraer y convertir valores (`extract_number`, `text_to_number`)

3. **Problema**: PySpark crea directorios en lugar de archivos CSV √∫nicos
   - **Soluci√≥n**: Uso de `coalesce(1)` y conversi√≥n a Pandas para guardar como archivo √∫nico

4. **Problema**: Permisos de escritura en Docker
   - **Soluci√≥n**: Configuraci√≥n de permisos en Dockerfile y script de inicio

5. **Problema**: Conflicto entre funciones de Python y PySpark (`sum`, `round`, `col`)
   - **Soluci√≥n**: Uso de `builtins.sum()` y `builtins.round()` para funciones de Python

### Mejoras Futuras

1. **Escalabilidad**: Migrar a Spark Cluster para datasets m√°s grandes
2. **Automatizaci√≥n**: Crear scripts de ETL automatizados con Airflow o similar
3. **Validaci√≥n**: Implementar tests unitarios para validar transformaciones
4. **Monitoreo**: Agregar logging y m√©tricas de calidad de datos
5. **Visualizaci√≥n**: Crear dashboards con los datos del Datawarehouse

---

## üìö Referencias

- [Documentaci√≥n de Pandas](https://pandas.pydata.org/docs/)
- [Documentaci√≥n de PySpark](https://spark.apache.org/docs/latest/api/python/)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
- [Docker Documentation](https://docs.docker.com/)
- [Data Warehouse Concepts](https://en.wikipedia.org/wiki/Data_warehouse)

---

## üë§ Autor

**Teo Aranda P√°ez**  
Proyecto RA1 - Big Data  
Diciembre 2024

---

## üìù Licencia

Este proyecto es parte de un trabajo acad√©mico para la Universidad La Salle.

